{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "27a65939",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import math\n",
    "import mmh3 \n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from tensorflow.keras.datasets import mnist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "73c81f05",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(60000, 28, 28) (60000,)\n",
      "(10000, 28, 28) (10000,)\n"
     ]
    }
   ],
   "source": [
    "(x_train, y_train), (x_test, y_test) = mnist.load_data()\n",
    "print(x_train.shape, y_train.shape)\n",
    "print(x_test.shape, y_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "ddadd33c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(28, 28)\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# plot some images\n",
    "img = x_train[0]\n",
    "label = y_train[0]\n",
    "print(img.shape)\n",
    "plt.imshow(img, cmap='gray')\n",
    "plt.title(label)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "bab42503",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 1, 28, 28])\n",
      "torch.Size([1, 128])\n"
     ]
    }
   ],
   "source": [
    "class SimpleEncoder(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(SimpleEncoder, self).__init__()\n",
    "        # A simple CNN encoder: input (batch_size,channels,height,width) -> output vector (batch_size,128)\n",
    "        self.conv = nn.Sequential(\n",
    "            nn.Conv2d(1, 16, kernel_size=3, stride=2, padding=1),  # 28x28 -> 14x14\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(16, 32, kernel_size=3, stride=2, padding=1), # 14x14 -> 7x7\n",
    "            nn.ReLU(),\n",
    "            nn.Flatten()\n",
    "        )\n",
    "        self.fc = nn.Linear(32 * 7 * 7, 128)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = self.conv(x)\n",
    "        x = self.fc(x)\n",
    "        return x  # output shape: (batch_size, 128)\n",
    "    \n",
    "    \n",
    "encoder = SimpleEncoder()\n",
    "x = x_train[0]\n",
    "x = torch.tensor(x, dtype=torch.float32).unsqueeze(0).unsqueeze(0)\n",
    "print(x.shape)\n",
    "emb = encoder(x)\n",
    "print(emb.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "8041b5a8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Memory matrix M after write:\n",
      "torch.Size([3, 2])\n",
      "Logits after read:\n",
      "torch.Size([1, 2])\n",
      "tensor([[-0.0991, -0.0684]], grad_fn=<AddmmBackward0>)\n",
      "Probabilities:\n",
      "torch.Size([1, 2])\n",
      "tensor([[0.4923, 0.5077]], grad_fn=<SoftmaxBackward0>)\n",
      "Predicted classes:\n",
      "tensor([1])\n"
     ]
    }
   ],
   "source": [
    "class NeuralBloomFilter(nn.Module):\n",
    "    def __init__(self, memory_slots=10, word_size=32, class_num=10):\n",
    "        \"\"\"\n",
    "        memory_slots: Number of memory slots (columns in memory matrix)\n",
    "        word_size: Dimension of the write word and query vector\n",
    "        \"\"\"\n",
    "        super(NeuralBloomFilter, self).__init__()\n",
    "        self.encoder = SimpleEncoder()\n",
    "        \n",
    "        # f_w: maps encoder output to a write word of dimension word_size\n",
    "        self.fc_w = nn.Linear(128, word_size)\n",
    "        \n",
    "        # f_q: maps encoder output to a query vector of dimension word_size\n",
    "        self.fc_q = nn.Linear(128, word_size)\n",
    "        \n",
    "        # Learnable addressing matrix A: shape (word_size, memory_slots)\n",
    "        self.A = nn.Parameter(torch.randn(word_size, memory_slots), requires_grad=True)\n",
    "        \n",
    "        # Memory matrix M: shape (memory_slots, word_size), stored as a buffer (non-trainable)\n",
    "        self.register_buffer('M', torch.zeros(memory_slots, word_size))\n",
    "\n",
    "        # MLP for final output: maps read vector to class_num\n",
    "        self.mlp = nn.Sequential(\n",
    "            nn.Linear(word_size, 128), # (batch_size, word_size) -> (batch_size, 128)\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(128, class_num) # (batch_size, 128) -> (batch_size, class_num)\n",
    "        )\n",
    " \n",
    "    def write(self, x):\n",
    "        \"\"\"\n",
    "        Write operation: Encode input x, generate write word w and query vector q,\n",
    "        compute addressing weights a via softmax(q @ A), then update memory: M = M + outer(w, a)\n",
    "        \"\"\"\n",
    "        z = self.encoder(x)         # (batch_size, 128)\n",
    "        w = self.fc_w(z)            # (batch_size, word_size)\n",
    "        q = self.fc_q(z)            # (batch_size, word_size)\n",
    "\n",
    "        a_logits = torch.matmul(q, self.A)  # (batch_size, memory_slots)\n",
    "        a = F.softmax(a_logits, dim=1)                   # (batch_size, memory_slots)\n",
    "        \n",
    "        \n",
    "        # a -> (batch_size, memory_slots) and w -> (batch_size, word_size)\n",
    "        # a.unsqueeze(2) -> (batch_size, memory_slots, 1)\n",
    "        # w.unsqueeze(1) -> (batch_size, 1, word_size)\n",
    "        update = torch.matmul(a.unsqueeze(2), w.unsqueeze(1)).detach()  # (batch_size, memory_slots, word_size)\n",
    "        \n",
    "        self.M = self.M + update.sum(dim=0) # (memory_slots, word_size)\n",
    "        \n",
    "        return a \n",
    "\n",
    "    def read(self, x):\n",
    "        \"\"\"\n",
    "        Read operation: Given input x, generate query vector q, compute addressing weights a, \n",
    "        then read from memory via weighted sum: read_vector = a @ M, resulting in a vector of dimension word_size.\n",
    "        \"\"\"\n",
    "        z = self.encoder(x)         # (batch_size, 128)\n",
    "        q = self.fc_q(z)            # (batch_size, word_size)\n",
    "        a_logits = torch.matmul(q, self.A)  # (batch_size, memory_slots)\n",
    "        a = F.softmax(a_logits, dim=1)                   # (batch_size, memory_slots)\n",
    "        # Read: weighted sum over memory slots:\n",
    "        read_vector = torch.matmul(a, self.M)       # (batch_size, word_size)\n",
    "        \n",
    "        logits = self.mlp(read_vector) # (batch_size, class_num)\n",
    "        return logits\n",
    "\n",
    "    def forward(self, x, mode='read'):\n",
    "        if mode == 'write':\n",
    "            return self.write(x)\n",
    "        else:\n",
    "            return self.read(x)\n",
    "        \n",
    "        \n",
    "# Example usage:\n",
    "model = NeuralBloomFilter(memory_slots=3, word_size=2, class_num=2)\n",
    "x = torch.randn(1, 1, 28, 28)\n",
    "\n",
    "model.write(x)\n",
    "\n",
    "# Print the memory matrix M after writing\n",
    "print(\"Memory matrix M after write:\")\n",
    "print(model.M.shape)\n",
    "\n",
    "# Read from the memory\n",
    "logits = model.read(x)\n",
    "print(\"Logits after read:\")\n",
    "print(logits.shape)\n",
    "print(logits)\n",
    "\n",
    "# Convert logits to probabilities\n",
    "probs = F.softmax(logits, dim=1)\n",
    "print(\"Probabilities:\")\n",
    "print(probs.shape)\n",
    "print(probs)\n",
    "\n",
    "# Final class predictions\n",
    "_, predicted_class = torch.max(probs, 1)\n",
    "print(\"Predicted classes:\")\n",
    "print(predicted_class)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "f125efbd",
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "def sample_task(labels, storage_set_size=10, num_queries=20):\n",
    "    class_to_indices = {}\n",
    "    for idx, label in enumerate(labels):\n",
    "        if label not in class_to_indices:\n",
    "            class_to_indices[label] = []\n",
    "        class_to_indices[label].append(idx)\n",
    "    \n",
    "    chosen_class = random.choice(list(class_to_indices.keys()))\n",
    "    # print(f\"Chosen class for storage set: {chosen_class}\")\n",
    "    storage_indices = random.sample(class_to_indices[chosen_class], storage_set_size)\n",
    "\n",
    "    num_in = num_queries // 2\n",
    "    num_out = num_queries - num_in\n",
    "    query_in_indices = random.sample(class_to_indices[chosen_class], num_in)\n",
    "    other_classes = [c for c in class_to_indices if c != chosen_class]\n",
    "    query_out_indices = []\n",
    "    for _ in range(num_out):\n",
    "        other_class = random.choice(other_classes)\n",
    "        query_out_indices.append(random.choice(class_to_indices[other_class]))\n",
    "    \n",
    "    query_indices = query_in_indices + query_out_indices\n",
    "    # define target as [1,0] for in-class and [0,1] for out-of-class\n",
    "    targets = []\n",
    "    for i in range(num_in):\n",
    "        targets.append([1, 0])\n",
    "    for i in range(num_out):\n",
    "        targets.append([0, 1])\n",
    "    targets = torch.tensor(targets, dtype=torch.float32)\n",
    "    \n",
    "    return storage_indices, query_indices, targets, class_to_indices, chosen_class\n",
    "\n",
    "storage_indices, query_indices, targets, class_to_indices, chosen_class = sample_task(y_train, storage_set_size=600, num_queries=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "8dd1a26e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def meta_train(model, dataset, labels, optimizer, criterion, device, meta_epochs=10, storage_set_size=60, num_queries=10):\n",
    "    model.train()\n",
    "    total_loss = 0.0\n",
    "    \n",
    "    for epoch in range(meta_epochs):\n",
    "        storage_indices, query_indices, targets, _, _ = sample_task(labels, storage_set_size, num_queries)\n",
    "        \n",
    "        # Load images for storage set S and query set Q.\n",
    "        storage_images = dataset[storage_indices]\n",
    "        # convert numpy array to tensor\n",
    "        storage_images = torch.tensor(storage_images, dtype=torch.float32).unsqueeze(1)\n",
    "        query_images = dataset[query_indices]\n",
    "        # convert numpy array to tensor\n",
    "        query_images = torch.tensor(query_images, dtype=torch.float32).unsqueeze(1)\n",
    "        \n",
    "        \n",
    "        # Reset model memory for the current task.\n",
    "        model.M.zero_()\n",
    "        \n",
    "        # Write phase: write all images in S into memory.\n",
    "        # (Assumes your write() method supports batch inputs.)\n",
    "        _ = model.write(storage_images)  # Expected shape: (storage_set_size, word_size)\n",
    "        \n",
    "        # Read phase: obtain logits for all query images.\n",
    "        logits = model.read(query_images)  # Expected shape: (num_queries, class_num)\n",
    "        # here logits has shape (num_queries, 2) and taregt has shape (num_queries,)\n",
    "        # Convert logits to probabilities using softmax.\n",
    "\n",
    "        # Compute cross-entropy loss (treating it as a binary classification: in-set vs. not in-set).\n",
    "        loss = criterion(logits, targets)\n",
    "        \n",
    "        # Backpropagation: compute gradients and update model parameters.\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        \n",
    "        total_loss += loss.item()\n",
    "        if (epoch + 1) % 10 == 0:\n",
    "            avg_loss = total_loss / (epoch + 1)\n",
    "            print(f\"Meta Epoch [{epoch+1}/{meta_epochs}], Loss: {loss.item():.4f}, Avg Loss: {avg_loss:.4f}\")\n",
    "    \n",
    "    return total_loss / meta_epochs\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "519a5c3e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Meta Epoch [10/1000], Loss: 146899.8750, Avg Loss: 53975.5620\n",
      "Meta Epoch [20/1000], Loss: 28246.6309, Avg Loss: 66007.0422\n",
      "Meta Epoch [30/1000], Loss: 3935.5393, Avg Loss: 55233.8351\n",
      "Meta Epoch [40/1000], Loss: 664.8325, Avg Loss: 43681.1856\n",
      "Meta Epoch [50/1000], Loss: 118.9805, Avg Loss: 35047.0681\n",
      "Meta Epoch [60/1000], Loss: 0.6947, Avg Loss: 29234.0816\n",
      "Meta Epoch [70/1000], Loss: 0.6938, Avg Loss: 25057.8834\n",
      "Meta Epoch [80/1000], Loss: 0.6935, Avg Loss: 21925.7347\n",
      "Meta Epoch [90/1000], Loss: 0.6933, Avg Loss: 19489.6190\n",
      "Meta Epoch [100/1000], Loss: 0.6933, Avg Loss: 17540.7264\n",
      "Meta Epoch [110/1000], Loss: 0.6932, Avg Loss: 15946.1780\n",
      "Meta Epoch [120/1000], Loss: 0.6932, Avg Loss: 14617.3876\n",
      "Meta Epoch [130/1000], Loss: 0.6932, Avg Loss: 13493.0265\n",
      "Meta Epoch [140/1000], Loss: 0.6932, Avg Loss: 12529.2884\n",
      "Meta Epoch [150/1000], Loss: 0.6932, Avg Loss: 11694.0487\n",
      "Meta Epoch [160/1000], Loss: 0.6932, Avg Loss: 10963.2140\n",
      "Meta Epoch [170/1000], Loss: 0.6932, Avg Loss: 10318.3598\n",
      "Meta Epoch [180/1000], Loss: 0.6932, Avg Loss: 9745.1561\n",
      "Meta Epoch [190/1000], Loss: 0.6932, Avg Loss: 9232.2896\n",
      "Meta Epoch [200/1000], Loss: 0.6931, Avg Loss: 8770.7098\n",
      "Meta Epoch [210/1000], Loss: 0.6931, Avg Loss: 8353.0900\n",
      "Meta Epoch [220/1000], Loss: 0.6931, Avg Loss: 7973.4356\n",
      "Meta Epoch [230/1000], Loss: 0.6931, Avg Loss: 7626.7946\n",
      "Meta Epoch [240/1000], Loss: 0.6931, Avg Loss: 7309.0404\n",
      "Meta Epoch [250/1000], Loss: 0.6931, Avg Loss: 7016.7065\n",
      "Meta Epoch [260/1000], Loss: 0.6931, Avg Loss: 6746.8598\n",
      "Meta Epoch [270/1000], Loss: 0.6931, Avg Loss: 6497.0018\n",
      "Meta Epoch [280/1000], Loss: 0.6931, Avg Loss: 6264.9908\n",
      "Meta Epoch [290/1000], Loss: 0.6931, Avg Loss: 6048.9805\n",
      "Meta Epoch [300/1000], Loss: 0.6931, Avg Loss: 5847.3709\n",
      "Meta Epoch [310/1000], Loss: 0.6931, Avg Loss: 5658.7684\n",
      "Meta Epoch [320/1000], Loss: 0.6931, Avg Loss: 5481.9536\n",
      "Meta Epoch [330/1000], Loss: 0.6931, Avg Loss: 5315.8548\n",
      "Meta Epoch [340/1000], Loss: 0.6931, Avg Loss: 5159.5265\n",
      "Meta Epoch [350/1000], Loss: 0.6931, Avg Loss: 5012.1312\n",
      "Meta Epoch [360/1000], Loss: 0.6931, Avg Loss: 4872.9246\n",
      "Meta Epoch [370/1000], Loss: 0.6931, Avg Loss: 4741.2427\n",
      "Meta Epoch [380/1000], Loss: 0.6931, Avg Loss: 4616.4914\n",
      "Meta Epoch [390/1000], Loss: 0.6931, Avg Loss: 4498.1376\n",
      "Meta Epoch [400/1000], Loss: 0.6931, Avg Loss: 4385.7015\n",
      "Meta Epoch [410/1000], Loss: 0.6931, Avg Loss: 4278.7501\n",
      "Meta Epoch [420/1000], Loss: 0.6931, Avg Loss: 4176.8916\n",
      "Meta Epoch [430/1000], Loss: 0.6931, Avg Loss: 4079.7707\n",
      "Meta Epoch [440/1000], Loss: 0.6931, Avg Loss: 3987.0644\n",
      "Meta Epoch [450/1000], Loss: 0.6931, Avg Loss: 3898.4783\n",
      "Meta Epoch [460/1000], Loss: 0.6931, Avg Loss: 3813.7439\n",
      "Meta Epoch [470/1000], Loss: 0.6931, Avg Loss: 3732.6151\n",
      "Meta Epoch [480/1000], Loss: 0.6931, Avg Loss: 3654.8668\n",
      "Meta Epoch [490/1000], Loss: 0.6931, Avg Loss: 3580.2918\n",
      "Meta Epoch [500/1000], Loss: 0.6931, Avg Loss: 3508.6998\n",
      "Meta Epoch [510/1000], Loss: 0.6931, Avg Loss: 3439.9154\n",
      "Meta Epoch [520/1000], Loss: 0.6931, Avg Loss: 3373.7765\n",
      "Meta Epoch [530/1000], Loss: 0.6931, Avg Loss: 3310.1334\n",
      "Meta Epoch [540/1000], Loss: 0.6931, Avg Loss: 3248.8475\n",
      "Meta Epoch [550/1000], Loss: 0.6931, Avg Loss: 3189.7901\n",
      "Meta Epoch [560/1000], Loss: 0.6931, Avg Loss: 3132.8420\n",
      "Meta Epoch [570/1000], Loss: 0.6931, Avg Loss: 3077.8920\n",
      "Meta Epoch [580/1000], Loss: 0.6931, Avg Loss: 3024.8368\n",
      "Meta Epoch [590/1000], Loss: 0.6931, Avg Loss: 2973.5801\n",
      "Meta Epoch [600/1000], Loss: 0.6931, Avg Loss: 2924.0320\n",
      "Meta Epoch [610/1000], Loss: 0.6931, Avg Loss: 2876.1084\n",
      "Meta Epoch [620/1000], Loss: 0.6931, Avg Loss: 2829.7308\n",
      "Meta Epoch [630/1000], Loss: 0.6931, Avg Loss: 2784.8254\n",
      "Meta Epoch [640/1000], Loss: 0.6931, Avg Loss: 2741.3234\n",
      "Meta Epoch [650/1000], Loss: 0.6931, Avg Loss: 2699.1598\n",
      "Meta Epoch [660/1000], Loss: 0.6931, Avg Loss: 2658.2740\n",
      "Meta Epoch [670/1000], Loss: 0.6931, Avg Loss: 2618.6086\n",
      "Meta Epoch [680/1000], Loss: 0.6931, Avg Loss: 2580.1098\n",
      "Meta Epoch [690/1000], Loss: 0.6931, Avg Loss: 2542.7270\n",
      "Meta Epoch [700/1000], Loss: 0.6931, Avg Loss: 2506.4122\n",
      "Meta Epoch [710/1000], Loss: 0.6931, Avg Loss: 2471.1204\n",
      "Meta Epoch [720/1000], Loss: 0.6931, Avg Loss: 2436.8089\n",
      "Meta Epoch [730/1000], Loss: 0.6931, Avg Loss: 2403.4374\n",
      "Meta Epoch [740/1000], Loss: 0.6931, Avg Loss: 2370.9679\n",
      "Meta Epoch [750/1000], Loss: 0.6931, Avg Loss: 2339.3643\n",
      "Meta Epoch [760/1000], Loss: 0.6931, Avg Loss: 2308.5923\n",
      "Meta Epoch [770/1000], Loss: 0.6931, Avg Loss: 2278.6196\n",
      "Meta Epoch [780/1000], Loss: 0.6931, Avg Loss: 2249.4154\n",
      "Meta Epoch [790/1000], Loss: 0.6931, Avg Loss: 2220.9505\n",
      "Meta Epoch [800/1000], Loss: 0.6931, Avg Loss: 2193.1973\n",
      "Meta Epoch [810/1000], Loss: 0.6931, Avg Loss: 2166.1294\n",
      "Meta Epoch [820/1000], Loss: 0.6931, Avg Loss: 2139.7216\n",
      "Meta Epoch [830/1000], Loss: 0.6931, Avg Loss: 2113.9502\n",
      "Meta Epoch [840/1000], Loss: 0.6931, Avg Loss: 2088.7924\n",
      "Meta Epoch [850/1000], Loss: 0.6931, Avg Loss: 2064.2265\n",
      "Meta Epoch [860/1000], Loss: 0.6931, Avg Loss: 2040.2319\n",
      "Meta Epoch [870/1000], Loss: 0.6931, Avg Loss: 2016.7889\n",
      "Meta Epoch [880/1000], Loss: 0.6931, Avg Loss: 1993.8788\n",
      "Meta Epoch [890/1000], Loss: 0.6931, Avg Loss: 1971.4834\n",
      "Meta Epoch [900/1000], Loss: 0.6931, Avg Loss: 1949.5857\n",
      "Meta Epoch [910/1000], Loss: 0.6931, Avg Loss: 1928.1693\n",
      "Meta Epoch [920/1000], Loss: 0.6931, Avg Loss: 1907.2185\n",
      "Meta Epoch [930/1000], Loss: 0.6931, Avg Loss: 1886.7182\n",
      "Meta Epoch [940/1000], Loss: 0.6931, Avg Loss: 1866.6541\n",
      "Meta Epoch [950/1000], Loss: 0.6931, Avg Loss: 1847.0124\n",
      "Meta Epoch [960/1000], Loss: 0.6931, Avg Loss: 1827.7800\n",
      "Meta Epoch [970/1000], Loss: 0.6931, Avg Loss: 1808.9440\n",
      "Meta Epoch [980/1000], Loss: 0.6931, Avg Loss: 1790.4925\n",
      "Meta Epoch [990/1000], Loss: 0.6931, Avg Loss: 1772.4137\n",
      "Meta Epoch [1000/1000], Loss: 0.6931, Avg Loss: 1754.6965\n",
      "Meta-training completed. Average Loss: 1754.6965\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import random\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    # Hyperparameters\n",
    "    meta_epochs = 1000  # Number of meta-tasks/episodes\n",
    "    storage_set_size = 500\n",
    "    num_queries = 200\n",
    "    learning_rate = 0.01\n",
    "\n",
    "    # Device configuration\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    \n",
    "    # MNIST dataset and transform\n",
    "    # Initialize your Neural Bloom Filter model (with desired parameters)\n",
    "    # Here, for demonstration, we use memory_slots=3, word_size=2, and class_num=2 (binary classification)\n",
    "    model = NeuralBloomFilter(memory_slots=3, word_size=32, class_num=2).to(device)\n",
    "    \n",
    "    # Use CrossEntropyLoss for binary classification (2 classes)\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    \n",
    "    # Use Adam optimizer\n",
    "    optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n",
    "    \n",
    "    # Start meta-training\n",
    "    avg_loss = meta_train(model, x_train, y_train,optimizer, criterion, device,\n",
    "                          meta_epochs=meta_epochs,\n",
    "                          storage_set_size=storage_set_size,\n",
    "                          num_queries=num_queries)\n",
    "    print(f\"Meta-training completed. Average Loss: {avg_loss:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ac91a77",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
