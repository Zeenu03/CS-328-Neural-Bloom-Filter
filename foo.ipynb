{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "27a65939",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import math\n",
    "import mmh3 \n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from tensorflow.keras.datasets import mnist\n",
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "73c81f05",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(60000, 28, 28) (60000,)\n",
      "(10000, 28, 28) (10000,)\n"
     ]
    }
   ],
   "source": [
    "(x_train, y_train), (x_test, y_test) = mnist.load_data()\n",
    "print(x_train.shape, y_train.shape)\n",
    "print(x_test.shape, y_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "bab42503",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SimpleEncoder(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(SimpleEncoder, self).__init__()\n",
    "        # A simple CNN encoder: input (batch_size,channels,height,width) -> output vector (batch_size,128)\n",
    "        self.conv = nn.Sequential(\n",
    "            nn.Conv2d(1, 16, kernel_size=3, stride=2, padding=1),  # 28x28 -> 14x14\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(16, 32, kernel_size=3, stride=2, padding=1), # 14x14 -> 7x7\n",
    "            nn.ReLU(),\n",
    "            nn.Flatten()\n",
    "        )\n",
    "        self.fc = nn.Linear(32 * 7 * 7, 128)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = self.conv(x)\n",
    "        x = self.fc(x)\n",
    "        return x  # output shape: (batch_size, 128)\n",
    "    \n",
    "class NeuralBloomFilter(nn.Module):\n",
    "    def __init__(self, memory_slots=10, word_size=32, class_num=1):\n",
    "        super().__init__()\n",
    "        self.encoder = SimpleEncoder()\n",
    "        self.fc_q    = nn.Linear(128, word_size)\n",
    "        self.fc_w    = nn.Linear(128, word_size)\n",
    "        self.A       = nn.Parameter(torch.randn(word_size, memory_slots))\n",
    "        self.register_buffer('M', torch.zeros(memory_slots, word_size))\n",
    "        inp_dim = memory_slots*word_size + word_size + 128\n",
    "        self.mlp = nn.Sequential(\n",
    "            nn.Linear(inp_dim, 128),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(128, class_num)\n",
    "        )\n",
    "\n",
    "    def controller(self, x):\n",
    "        \"\"\"\n",
    "        Runs the encoder, computes query q, write word w, and normalized address a.\n",
    "        Returns (a, w, z).\n",
    "        \"\"\"\n",
    "        z = self.encoder(x)                   # (B,128)\n",
    "        q = self.fc_q(z)                      # (B,word_size)\n",
    "        w = self.fc_w(z)                      # (B,word_size)\n",
    "        a_logits = q @ self.A                 # (B,slots)\n",
    "        a = F.softmax(a_logits, dim=1)        # (B,slots)\n",
    "        return a, w, z\n",
    "\n",
    "    def write(self, x):\n",
    "        \"\"\"\n",
    "        Write all x in one shot:\n",
    "          M ← M + ∑_i w_i a_i^T\n",
    "        \"\"\"\n",
    "        a, w, _ = self.controller(x)          # a:(B,slots), w:(B,word_size)\n",
    "        # Outer product per sample: update_i[k,p] = a[i,k] * w[i,p]\n",
    "        # Stack them and sum over batch:\n",
    "        # shape: (B, slots, word_size)\n",
    "        update = torch.einsum('bk,bp->bkp', a, w)\n",
    "        # Add to memory and detach so writes don't backprop through time:\n",
    "        self.M = self.M + update.sum(dim=0).detach()\n",
    "\n",
    "    def read(self, x):\n",
    "        \"\"\"\n",
    "        Read operation:\n",
    "          r_i = flatten( M ⊙ a_i )  (componentwise)\n",
    "          logits = f_out([r_i, w_i, z_i])\n",
    "        \"\"\"\n",
    "        a, w, z = self.controller(x)          # (B,slots), (B,word_size), (B,128)\n",
    "        # M ⊙ a_i: scale each row of M by a_i:\n",
    "        # shape before flatten: (B, slots, word_size)\n",
    "        r = (a.unsqueeze(2) * self.M.unsqueeze(0)).reshape(x.size(0), -1)  # (B, slots*word_size)\n",
    "        # Concatenate r, w, z:\n",
    "        concat = torch.cat([r, w, z], dim=1)                              # (B, inp_dim)\n",
    "        logits = self.mlp(concat)                                          # (B, class_num)\n",
    "        return logits\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f125efbd",
   "metadata": {},
   "outputs": [],
   "source": [
    "def sample_task(labels, storage_set_size, num_queries, class_num):\n",
    "    class_to_indices = {}\n",
    "    for idx, label in enumerate(labels):\n",
    "        if label not in class_to_indices:\n",
    "            class_to_indices[label] = []\n",
    "        class_to_indices[label].append(idx)\n",
    "    \n",
    "    storage_indices = random.sample(class_to_indices[class_num], storage_set_size)\n",
    "\n",
    "    num_in = num_queries // 2\n",
    "    num_out = num_queries - num_in\n",
    "    query_in_indices = random.sample(class_to_indices[class_num], num_in)\n",
    "    other_classes = [c for c in class_to_indices if c != class_num]\n",
    "    query_out_indices = []\n",
    "    for _ in range(num_out):\n",
    "        other_class = random.choice(other_classes)\n",
    "        query_out_indices.append(random.choice(class_to_indices[other_class]))\n",
    "    \n",
    "    query_indices = query_in_indices + query_out_indices\n",
    "    # define target as [1,0] for in-class and [0,1] for out-of-class\n",
    "    targets = []\n",
    "    for i in range(num_in):\n",
    "        targets.append(1)\n",
    "    for i in range(num_out):\n",
    "        targets.append(0)\n",
    "    \n",
    "    targets = torch.tensor(targets, dtype=torch.float32).unsqueeze(1) \n",
    "    return storage_indices, query_indices, targets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "8dd1a26e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def meta_train(model, dataset, labels, optimizer, criterion, device, meta_epochs=10, storage_set_size=60, num_queries=10):\n",
    "    \n",
    "    model.train()\n",
    "    total_loss = 0.0\n",
    "    classes = [0,1,2,3,4,5,6]\n",
    "\n",
    "    for epoch in range(meta_epochs):\n",
    "        \n",
    "        class_num = random.choice(classes)\n",
    "        storage_indices, query_indices, targets= sample_task(labels, storage_set_size, num_queries, class_num)\n",
    "        storage_images = dataset[storage_indices]\n",
    "        storage_images = torch.tensor(storage_images, dtype=torch.float32).unsqueeze(1)\n",
    "        query_images = dataset[query_indices]\n",
    "        query_images = torch.tensor(query_images, dtype=torch.float32).unsqueeze(1)\n",
    "        \n",
    "        model.M.zero_()\n",
    "        \n",
    "        _ = model.write(storage_images)  # Expected shape: (storage_set_size, word_size)\n",
    "        \n",
    "        logits = model.read(query_images)  # Expected shape: (num_queries, class_num)\n",
    "        \n",
    "        \n",
    "        probs = torch.sigmoid(logits) \n",
    "        \n",
    "        # Convert probabilities to binary predictions\n",
    "        predictions = (probs > 0.5).float()\n",
    "        \n",
    "        fnr = 0\n",
    "        fpr = 0\n",
    "        for i in range(len(predictions)):\n",
    "            if predictions[i] == 0 and targets[i] == 1:\n",
    "                fnr += 1\n",
    "            elif predictions[i] == 1 and targets[i] == 0:\n",
    "                fpr += 1\n",
    "        \n",
    "        loss = criterion(logits, targets)\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        \n",
    "        total_loss += loss.item()\n",
    "        if (epoch + 1) % 100 == 0:\n",
    "            avg_loss = total_loss / (epoch + 1)\n",
    "            print(f\"Epoch [{epoch+1}/{meta_epochs}], Loss: {loss.item():.4f}, False Positive Rate: {fpr}, False Negative Rate: {fnr}\")\n",
    "    \n",
    "    return total_loss / meta_epochs\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "519a5c3e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [100/2000], Loss: 0.9922, False Positive Rate: 17, False Negative Rate: 65\n",
      "Epoch [200/2000], Loss: 0.6182, False Positive Rate: 21, False Negative Rate: 44\n",
      "Epoch [300/2000], Loss: 0.6598, False Positive Rate: 35, False Negative Rate: 33\n",
      "Epoch [400/2000], Loss: 0.5731, False Positive Rate: 25, False Negative Rate: 37\n",
      "Epoch [500/2000], Loss: 0.6020, False Positive Rate: 23, False Negative Rate: 29\n",
      "Epoch [600/2000], Loss: 0.4749, False Positive Rate: 25, False Negative Rate: 18\n",
      "Epoch [700/2000], Loss: 0.7805, False Positive Rate: 11, False Negative Rate: 25\n",
      "Epoch [800/2000], Loss: 0.6146, False Positive Rate: 26, False Negative Rate: 34\n",
      "Epoch [900/2000], Loss: 0.5845, False Positive Rate: 22, False Negative Rate: 29\n",
      "Epoch [1000/2000], Loss: 0.5398, False Positive Rate: 21, False Negative Rate: 29\n",
      "Epoch [1100/2000], Loss: 0.5020, False Positive Rate: 18, False Negative Rate: 20\n",
      "Epoch [1200/2000], Loss: 0.4031, False Positive Rate: 17, False Negative Rate: 16\n",
      "Epoch [1300/2000], Loss: 0.7764, False Positive Rate: 15, False Negative Rate: 21\n",
      "Epoch [1400/2000], Loss: 0.3726, False Positive Rate: 4, False Negative Rate: 14\n",
      "Epoch [1500/2000], Loss: 0.3996, False Positive Rate: 13, False Negative Rate: 20\n",
      "Epoch [1600/2000], Loss: 0.5016, False Positive Rate: 15, False Negative Rate: 29\n",
      "Epoch [1700/2000], Loss: 0.4233, False Positive Rate: 21, False Negative Rate: 14\n",
      "Epoch [1800/2000], Loss: 0.1698, False Positive Rate: 5, False Negative Rate: 3\n",
      "Epoch [1900/2000], Loss: 0.7156, False Positive Rate: 12, False Negative Rate: 18\n",
      "Epoch [2000/2000], Loss: 0.1459, False Positive Rate: 0, False Negative Rate: 8\n",
      "Meta-training completed. Average Loss: 0.5475\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import random\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    # Hyperparameters\n",
    "    meta_epochs = 2000\n",
    "    storage_set_size = 1000\n",
    "    num_queries = 200\n",
    "    learning_rate = 1e-5\n",
    "    \n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    \n",
    "    model = NeuralBloomFilter(memory_slots=64, word_size=64, class_num=1).to(device)\n",
    "    \n",
    "    # Use CrossEntropyLoss for binary classification (2 classes)\n",
    "    criterion = nn.BCEWithLogitsLoss()\n",
    "    \n",
    "    # Use Adam optimizer\n",
    "    optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n",
    "    \n",
    "    # Start meta-training\n",
    "    avg_loss = meta_train(model, x_train, y_train,optimizer, criterion, device,\n",
    "                          meta_epochs=meta_epochs,\n",
    "                          storage_set_size=storage_set_size,\n",
    "                          num_queries=num_queries)\n",
    "    print(f\"Meta-training completed. Average Loss: {avg_loss:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "1ac91a77",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Accuracy: 0.9020\n"
     ]
    }
   ],
   "source": [
    "x_test = torch.tensor(x_test, dtype=torch.float32).unsqueeze(1)\n",
    "y_labels = [0]*len(x_test)\n",
    "\n",
    "\n",
    "for i in range(len(x_test)):\n",
    "    if y_test[i] in [0]:\n",
    "        y_labels[i] = 1\n",
    "    else:\n",
    "        y_labels[i] = 0\n",
    "        \n",
    "y_labels = torch.tensor(y_labels, dtype=torch.float32).unsqueeze(1)\n",
    "\n",
    "logits = model.read(x_test)  # Expected shape: (num_queries, class_num)\n",
    "probs = F.softmax(logits, dim=1)  # Convert logits to probabilities of shape (num_queries, 2)\n",
    "predicted_class = torch.argmax(probs, dim=1)\n",
    "accuracy = (predicted_class == y_labels[:, 0].long()).float().mean().item()\n",
    "print(f\"Test Accuracy: {accuracy:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "442c3244",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
