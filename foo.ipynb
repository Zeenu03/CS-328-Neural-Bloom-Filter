{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 97,
   "id": "27a65939",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from tensorflow.keras.datasets import mnist\n",
    "import random\n",
    "import mmh3\n",
    "from bitarray import bitarray"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "id": "73c81f05",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(60000, 28, 28) (60000,)\n",
      "(10000, 28, 28) (10000,)\n"
     ]
    }
   ],
   "source": [
    "(x_train, y_train), (x_test, y_test) = mnist.load_data()\n",
    "print(x_train.shape, y_train.shape)\n",
    "print(x_test.shape, y_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "id": "bab42503",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SimpleEncoder(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(SimpleEncoder, self).__init__()\n",
    "        # A simple CNN encoder: input (batch_size,channels,height,width) -> output vector (batch_size,128)\n",
    "        self.conv = nn.Sequential(\n",
    "            nn.Conv2d(1, 16, kernel_size=3, stride=2, padding=1),  # 28x28 -> 14x14\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(16, 32, kernel_size=3, stride=2, padding=1), # 14x14 -> 7x7\n",
    "            nn.ReLU(),\n",
    "            nn.Flatten()\n",
    "        )\n",
    "        self.fc = nn.Linear(32 * 7 * 7, 128)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = self.conv(x)\n",
    "        x = self.fc(x)\n",
    "        return x  # output shape: (batch_size, 128)\n",
    "    \n",
    "class NeuralBloomFilter(nn.Module):\n",
    "    def __init__(self, memory_slots=10, word_size=32, class_num=1):\n",
    "        super().__init__()\n",
    "        self.encoder = SimpleEncoder()\n",
    "        self.fc_q    = nn.Linear(128, word_size)\n",
    "        self.fc_w    = nn.Linear(128, word_size)\n",
    "        self.A       = nn.Parameter(torch.randn(word_size, memory_slots))\n",
    "        self.register_buffer('M', torch.zeros(memory_slots, word_size))\n",
    "        inp_dim = memory_slots*word_size + word_size + 128\n",
    "        self.mlp = nn.Sequential(\n",
    "            nn.Linear(inp_dim, 128),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(128, class_num)\n",
    "        )\n",
    "\n",
    "    def controller(self, x):\n",
    "        \"\"\"\n",
    "        Runs the encoder, computes query q, write word w, and normalized address a.\n",
    "        Returns (a, w, z).\n",
    "        \"\"\"\n",
    "        z = self.encoder(x)                   # (B,128)\n",
    "        q = self.fc_q(z)                      # (B,word_size)\n",
    "        w = self.fc_w(z)                      # (B,word_size)\n",
    "        a_logits = q @ self.A                 # (B,slots)\n",
    "        a = F.softmax(a_logits, dim=1)        # (B,slots)\n",
    "        return a, w, z\n",
    "\n",
    "    def write(self, x):\n",
    "        \"\"\"\n",
    "        Write all x in one shot:\n",
    "          M ← M + ∑_i w_i a_i^T\n",
    "        \"\"\"\n",
    "        a, w, _ = self.controller(x)          # a:(B,slots), w:(B,word_size)\n",
    "        # Outer product per sample: update_i[k,p] = a[i,k] * w[i,p]\n",
    "        # Stack them and sum over batch:\n",
    "        # shape: (B, slots, word_size)\n",
    "        update = torch.einsum('bk,bp->bkp', a, w)\n",
    "        # Add to memory and detach so writes don't backprop through time:\n",
    "        self.M = self.M + update.sum(dim=0).detach()\n",
    "\n",
    "    def read(self, x):\n",
    "        \"\"\"\n",
    "        Read operation:\n",
    "          r_i = flatten( M ⊙ a_i )  (componentwise)\n",
    "          logits = f_out([r_i, w_i, z_i])\n",
    "        \"\"\"\n",
    "        a, w, z = self.controller(x)          # (B,slots), (B,word_size), (B,128)\n",
    "        # M ⊙ a_i: scale each row of M by a_i:\n",
    "        # shape before flatten: (B, slots, word_size)\n",
    "        r = (a.unsqueeze(2) * self.M.unsqueeze(0)).reshape(x.size(0), -1)  # (B, slots*word_size)\n",
    "        # Concatenate r, w, z:\n",
    "        concat = torch.cat([r, w, z], dim=1)                              # (B, inp_dim)\n",
    "        logits = self.mlp(concat)                                          # (B, class_num)\n",
    "        return logits\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "id": "d8aeee6c",
   "metadata": {},
   "outputs": [],
   "source": [
    "class BloomFilter:\n",
    "    \"\"\"\n",
    "    Standard Bloom Filter with customizable size and hash count.\n",
    "    \"\"\"\n",
    "    def __init__(self, m: int, k: int):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            m: Number of bits in the filter bit array.\n",
    "            k: Number of hash functions.\n",
    "        \"\"\"\n",
    "        self.m = m\n",
    "        self.k = k\n",
    "        self.bits = bitarray(m)\n",
    "        self.bits.setall(0)\n",
    "\n",
    "    def add(self, key_bytes: bytes):\n",
    "        \"\"\"Insert a key (bytes) into the Bloom filter.\"\"\"\n",
    "        for i in range(self.k):\n",
    "            idx = mmh3.hash(key_bytes, i) % self.m\n",
    "            self.bits[idx] = 1\n",
    "\n",
    "    def __contains__(self, key_bytes: bytes) -> bool:\n",
    "        \"\"\"Check membership of a key (bytes).\"\"\"\n",
    "        return all(self.bits[mmh3.hash(key_bytes, i) % self.m]\n",
    "                   for i in range(self.k))\n",
    "\n",
    "class SimHashLSH:\n",
    "    \"\"\"\n",
    "    Locality-Sensitive Hashing via random hyperplanes (SimHash).\n",
    "    \"\"\"\n",
    "    def __init__(self, dim: int = 128, bits: int = 64):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            dim: Dimensionality of input embeddings.\n",
    "            bits: Number of bits in the SimHash fingerprint.\n",
    "        \"\"\"\n",
    "        # Random hyperplanes drawn from Gaussian distribution\n",
    "        self.hyperplanes = np.random.randn(bits, dim).astype(np.float32)\n",
    "\n",
    "    def compute(self, emb: np.ndarray) -> bytes:\n",
    "        \"\"\"Compute a `bits`-bit SimHash fingerprint for the embedding.\"\"\"\n",
    "        projections = np.dot(self.hyperplanes, emb)  # shape (bits,)\n",
    "        bits_arr = (projections >= 0).astype(np.uint8)  # 1 if >=0, else 0\n",
    "        packed = np.packbits(bits_arr)  # pack each 8 bits into a byte\n",
    "        return packed.tobytes()         # result length = bits//8\n",
    "\n",
    "class BackupBloomFilter:\n",
    "    \"\"\"\n",
    "    Combines a learned classifier or embedding-based filter with a backup Bloom filter.\n",
    "    \"\"\"\n",
    "    def __init__(self, encoder, bbf_size: int, bbf_hashes: int,\n",
    "                 lsh_bits: int = 64):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            encoder: Function or model that maps an input image -> 1D embedding (np.ndarray, shape (dim,)).\n",
    "            classifier: Function or model that maps embedding -> bool (returns True if \"in set\").\n",
    "            bbf_size: Number of bits in the backup Bloom filter.\n",
    "            bbf_hashes: Number of hash functions for the BBF.\n",
    "            lsh_bits: Number of bits in SimHash fingerprint.\n",
    "        \"\"\"\n",
    "        self.encoder = encoder\n",
    "        self.lsh = SimHashLSH(dim=encoder.fc.out_features, bits=lsh_bits)\n",
    "        self.bbf = BloomFilter(m=bbf_size, k=bbf_hashes)\n",
    "\n",
    "    def insert(self, image):\n",
    "        \"\"\"Insert an image into the backup filter (and optionally train classifier).\"\"\"\n",
    "        emb_t = self.encoder(image)          # expects np.ndarray shape (B, dim)\n",
    "        emb = emb_t.squeeze(0).detach().cpu().numpy()\n",
    "        key = self.lsh.compute(emb)\n",
    "        self.bbf.add(key)\n",
    "        # Optionally: classifier.fit or online update here\n",
    "\n",
    "    def query(self, image) -> bool:\n",
    "        \"\"\"Query the learned filter, fallback to BBF for zero false negatives.\"\"\"\n",
    "        emb_t = self.encoder(image) # expects np.ndarray shape (B, dim)\n",
    "        emb = emb_t.squeeze(0).detach().cpu().numpy()\n",
    "        key = self.lsh.compute(emb)\n",
    "        return (key in self.bbf)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "id": "f125efbd",
   "metadata": {},
   "outputs": [],
   "source": [
    "def sample_task(labels, storage_set_size, num_queries, class_num):\n",
    "    class_to_indices = {}\n",
    "    for idx, label in enumerate(labels):\n",
    "        if label not in class_to_indices:\n",
    "            class_to_indices[label] = []\n",
    "        class_to_indices[label].append(idx)\n",
    "    \n",
    "    storage_indices = random.sample(class_to_indices[class_num], storage_set_size)\n",
    "\n",
    "    num_in = num_queries // 2\n",
    "    num_out = num_queries - num_in\n",
    "    query_in_indices = random.sample(class_to_indices[class_num], num_in)\n",
    "    other_classes = [c for c in class_to_indices if c != class_num]\n",
    "    query_out_indices = []\n",
    "    for _ in range(num_out):\n",
    "        other_class = random.choice(other_classes)\n",
    "        query_out_indices.append(random.choice(class_to_indices[other_class]))\n",
    "    \n",
    "    query_indices = query_in_indices + query_out_indices\n",
    "    # define target as [1,0] for in-class and [0,1] for out-of-class\n",
    "    targets = []\n",
    "    for i in range(num_in):\n",
    "        targets.append(1)\n",
    "    for i in range(num_out):\n",
    "        targets.append(0)\n",
    "    \n",
    "    targets = torch.tensor(targets, dtype=torch.float32).unsqueeze(1) \n",
    "    return storage_indices, query_indices, targets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "id": "8dd1a26e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def meta_train(model, dataset, labels, optimizer, backupBloomfilter, criterion, device, meta_epochs=10, storage_set_size=60, num_queries=10):\n",
    "    \n",
    "    model.train()\n",
    "    total_loss = 0.0\n",
    "    classes = [0]\n",
    "\n",
    "    for epoch in range(meta_epochs):\n",
    "        \n",
    "        class_num = random.choice(classes)\n",
    "        storage_indices, query_indices, targets= sample_task(labels, storage_set_size, num_queries, class_num)\n",
    "        storage_images = dataset[storage_indices]\n",
    "        storage_images = torch.tensor(storage_images, dtype=torch.float32).unsqueeze(1)\n",
    "        query_images = dataset[query_indices]\n",
    "        query_images = torch.tensor(query_images, dtype=torch.float32).unsqueeze(1)\n",
    "        \n",
    "        model.M.zero_()\n",
    "        backupBloomfilter.bbf.bits.setall(0)\n",
    "        _ = model.write(storage_images)  # Expected shape: (storage_set_size, word_size)\n",
    "        \n",
    "        # Write to the backup Bloom filter\n",
    "        for i in range(storage_set_size):\n",
    "            \n",
    "            backupBloomfilter.insert(storage_images[i].unsqueeze(0))\n",
    "        \n",
    "        \n",
    "        logits = model.read(query_images)  # Expected shape: (num_queries, class_num)\n",
    "        \n",
    "        \n",
    "        probs = torch.sigmoid(logits) \n",
    "        \n",
    "        # Convert probabilities to binary predictions\n",
    "        predictions = (probs > 0.5).float()\n",
    "        \n",
    "        \n",
    "        for i in range(len(predictions)):\n",
    "            if predictions[i] == 0 and targets[i] == 1:\n",
    "                predictions[i] = backupBloomfilter.query(query_images[i].unsqueeze(0))\n",
    "        \n",
    "        fnr = 0\n",
    "        fpr = 0\n",
    "        for i in range(len(predictions)):\n",
    "            if predictions[i] == 0 and targets[i] == 1:\n",
    "                fnr += 1\n",
    "            elif predictions[i] == 1 and targets[i] == 0:\n",
    "                fpr += 1\n",
    "        \n",
    "        loss = criterion(logits, targets)\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        \n",
    "        total_loss += loss.item()\n",
    "        if (epoch + 1) % 10 == 0:\n",
    "            avg_loss = total_loss / (epoch + 1)\n",
    "            print(f\"Epoch [{epoch+1}/{meta_epochs}], Loss: {loss.item():.4f}, False Positive Rate: {fpr}, False Negative Rate: {fnr}\")\n",
    "    \n",
    "    return total_loss / meta_epochs\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "id": "519a5c3e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [10/100], Loss: 1.2477, False Positive Rate: 8, False Negative Rate: 1\n",
      "Epoch [20/100], Loss: 1.0488, False Positive Rate: 4, False Negative Rate: 3\n",
      "Epoch [30/100], Loss: 0.5281, False Positive Rate: 2, False Negative Rate: 2\n",
      "Epoch [40/100], Loss: 0.4766, False Positive Rate: 2, False Negative Rate: 0\n",
      "Epoch [50/100], Loss: 0.7468, False Positive Rate: 5, False Negative Rate: 2\n",
      "Epoch [60/100], Loss: 0.5182, False Positive Rate: 0, False Negative Rate: 4\n",
      "Epoch [70/100], Loss: 0.6591, False Positive Rate: 2, False Negative Rate: 2\n",
      "Epoch [80/100], Loss: 0.2813, False Positive Rate: 0, False Negative Rate: 0\n",
      "Epoch [90/100], Loss: 0.3495, False Positive Rate: 2, False Negative Rate: 2\n",
      "Epoch [100/100], Loss: 0.4146, False Positive Rate: 2, False Negative Rate: 2\n",
      "Meta-training completed. Average Loss: 0.5564\n",
      "bitarray('01001110101011101111110011010001101011001101111101100111110101011110101101111101100101111101110010101101111000010110100110011110111010100000000010101100100101111011011001011100010101111111001000011001111100011101101110100110001111001000111000010101011111110010011101011001010010111101100010000011011001110001111110000110011011001110110010101011111111010111011110010001000111101001100111010010011100101000110011111000111010010000011010110010000101111111000000000010101010101101011101000000111101111010010000110011111000111011101001101101101010111000000111011000011010010010010001110101101011101111110100000111111101000101111110110111101111111101101110111100010000010111111101011100000100011111101101011101110010100001011100011111010011111000001110001110111100111110011001010101011010110000110011010010100110101111111100010110010111011100011001000101101001001111111101110010010000001111001100101010001011101000101000000111110011011110110001101100110011111010100100101001111100011100010000011001111110101101100010101100110111111101010010111100101111010110101010011100001111010011011101010100001011001101011111110110001101010111101001101100101100110010010011111010100110001110010101000000011000011111011110100101100100010100101111110111110011000110100101111100011000010001010011111101001011111011111010000101111010101011101111100100011000100001011010001100111100100111011001010110111000010110110111111101110100011100110101011001000111111001101001010011100101111111111100110011010010001011100101100101101100011011110010111101011111001101001011101111011001100011000010010011101001000010001101100111111100011010001001001101111001001110110001100110000011101110000101110101010011001010011011101011010000011101100111100111000011110111110101111111100011011100100010111001011010100000000001101110101011011011111100011101000111001000110101100001110100011001110101101100101100001011111010000101000111110010110001111110010101110011111010000111101110101011101010000000101100010100011110111110111011101111011110110010')\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import random\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    # Hyperparameters\n",
    "    meta_epochs = 100\n",
    "    storage_set_size = 100\n",
    "    num_queries = 20\n",
    "    learning_rate = 1e-5\n",
    "    \n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    \n",
    "    model = NeuralBloomFilter(memory_slots=64, word_size=64, class_num=1).to(device)\n",
    "    \n",
    "    BackupBloomFilter = BackupBloomFilter(\n",
    "        encoder=model.encoder,\n",
    "        bbf_size=2000,  # Size of the backup Bloom filter\n",
    "        bbf_hashes=16,   # Number of hash functions for the BBF\n",
    "        lsh_bits=32    # Number of bits in the SimHash fingerprint\n",
    "    )\n",
    "    \n",
    "    # Use CrossEntropyLoss for binary classification (2 classes)\n",
    "    criterion = nn.BCEWithLogitsLoss()\n",
    "    \n",
    "    # Use Adam optimizer\n",
    "    optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n",
    "    \n",
    "    # Start meta-training\n",
    "    avg_loss = meta_train(model, x_train, y_train,optimizer, BackupBloomFilter,criterion, device,\n",
    "                          meta_epochs=meta_epochs,\n",
    "                          storage_set_size=storage_set_size,\n",
    "                          num_queries=num_queries)\n",
    "    print(f\"Meta-training completed. Average Loss: {avg_loss:.4f}\")\n",
    "    # print the backup bloom filter\n",
    "    print(BackupBloomFilter.bbf.bits)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "id": "1ac91a77",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Accuracy: 0.9020\n"
     ]
    }
   ],
   "source": [
    "x_test = torch.tensor(x_test, dtype=torch.float32).unsqueeze(1)\n",
    "y_labels = [0]*len(x_test)\n",
    "\n",
    "\n",
    "for i in range(len(x_test)):\n",
    "    if y_test[i] in [0]:\n",
    "        y_labels[i] = 1\n",
    "    else:\n",
    "        y_labels[i] = 0\n",
    "        \n",
    "y_labels = torch.tensor(y_labels, dtype=torch.float32).unsqueeze(1)\n",
    "\n",
    "logits = model.read(x_test)  # Expected shape: (num_queries, class_num)\n",
    "probs = F.softmax(logits, dim=1)  # Convert logits to probabilities of shape (num_queries, 2)\n",
    "predicted_class = torch.argmax(probs, dim=1)\n",
    "accuracy = (predicted_class == y_labels[:, 0].long()).float().mean().item()\n",
    "print(f\"Test Accuracy: {accuracy:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "442c3244",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
